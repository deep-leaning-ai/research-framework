# Research ML Framework

í†µí•© ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í—˜ í”„ë ˆì„ì›Œí¬ - Transfer Learningê³¼ ì¼ë°˜ ML íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì™„ë²½í•œ ì†”ë£¨ì…˜

## ì£¼ìš” íŠ¹ì§•

### ğŸ”¥ Transfer Learning ì§€ì›
- **ResNet** (18, 34, 50, 101, 152)
- **VGG** (11, 13, 16, 19 + BatchNorm ë³€í˜•)
- 3ê°€ì§€ í•™ìŠµ ì „ëµ: `feature_extraction`, `fine_tuning`, `inference`

### ğŸ“Š ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›
- **Multi-Class Classification** - Softmax + CrossEntropy
- **Binary Classification** - Sigmoid + BCEWithLogits
- **Regression** - MSE/MAE/R2 ë©”íŠ¸ë¦­

### ğŸ“ˆ ê³ ê¸‰ ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ
- 9ê°€ì§€ ë©”íŠ¸ë¦­: Accuracy, Precision, Recall, F1, Top5, AUC, MSE, MAE, R2
- ì‹¤ì‹œê°„ ì¶”ì  ë° ì´ë™í‰ê· 
- ë©”íŠ¸ë¦­ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

### ğŸ¨ ì‹œê°í™” ë„êµ¬
- 8-panel ì¢…í•© ì°¨íŠ¸
- í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ê³¡ì„ 
- ëª¨ë¸ ë¹„êµ ë¶„ì„
- íš¨ìœ¨ì„± ì‚°ì ë„

### âš¡ ì„±ëŠ¥ ìµœì í™”
- ìë™ GPU ê°ì§€ ë° í™œìš©
- DataLoader ìµœì í™” (persistent_workers, prefetch, pin_memory)
- ë°°ì¹˜ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ

## ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install -e .

# ì „ì²´ ê¸°ëŠ¥ ì„¤ì¹˜ (wandb, jupyter í¬í•¨)
pip install -e ".[all]"

# ê°œë°œ í™˜ê²½ ì„¤ì¹˜
pip install -e ".[dev]"
```

## ë¹ ë¥¸ ì‹œì‘

### 1. Transfer Learning ì˜ˆì œ

```python
import research
from research.data.cifar10 import CIFAR10DataModule

# ë°ì´í„° ì¤€ë¹„
data_module = CIFAR10DataModule(batch_size=32, num_workers=4)
train_loader, val_loader, test_loader = data_module.get_loaders()

# ì‹¤í—˜ ì„¤ì •
config = {
    'num_classes': 10,
    'learning_rate': 1e-4,
    'max_epochs': 20,
    'batch_size': 32
}

# ì‹¤í—˜ ìƒì„± ë° ì‹¤í–‰
exp = research.Experiment(config)
exp.setup(
    model_name='resnet18',
    data_module=data_module,
    freeze_strategy='fine_tuning'
)

# í•™ìŠµ
result = exp.run()
print(f"Best accuracy: {result.best_test_metric:.4f}")
```

### 2. ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ

```python
# ì—¬ëŸ¬ ì „ëµ ë¹„êµ
results = exp.compare_strategies(['feature_extraction', 'fine_tuning'])

# ì‹œê°í™”
from research.visualization import ExperimentVisualizer
ExperimentVisualizer.plot_comparison(results, save_path='comparison.png')
```

### 3. ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì¶”ê°€

```python
from research.metrics.base import BaseMetric

class CustomMetric(BaseMetric):
    def calculate(self, predictions, targets):
        # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ë¡œì§
        return metric_value

# ì‚¬ìš©
from research.metrics.tracker import MetricTracker
tracker = MetricTracker(window_size=5)
metrics = {'custom': CustomMetric()}
tracker.update(predictions, targets, metrics)
```

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
research/
â”œâ”€â”€ core/              # ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pretrained/   # ResNet, VGG (Transfer Learning)
â”‚   â””â”€â”€ simple/       # CNN, FullyConnected
â”œâ”€â”€ strategies/
â”‚   â”œâ”€â”€ training/     # í•™ìŠµ ì „ëµ
â”‚   â”œâ”€â”€ logging/      # ë¡œê¹… ì „ëµ (Simple, WandB)
â”‚   â””â”€â”€ task/         # íƒœìŠ¤í¬ ì „ëµ (MultiClass, Binary, Regression)
â”œâ”€â”€ metrics/          # ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ
â”œâ”€â”€ experiment/       # ì‹¤í—˜ ê´€ë¦¬
â”œâ”€â”€ comparison/       # ëª¨ë¸ ë¹„êµ
â”œâ”€â”€ visualization/    # ì‹œê°í™” ë„êµ¬
â”œâ”€â”€ data/            # ë°ì´í„° ëª¨ë“ˆ
â””â”€â”€ config/          # ì„¤ì • ê´€ë¦¬
```

## ë””ìì¸ íŒ¨í„´

- **Strategy Pattern**: íƒœìŠ¤í¬, í•™ìŠµ, ë¡œê¹… ì „ëµ
- **Factory Pattern**: ModelRegistryë¥¼ í†µí•œ ëª¨ë¸ ìƒì„±
- **Template Method**: BaseModelì˜ ê³µí†µ ë¡œì§
- **Facade Pattern**: Experiment í´ë˜ìŠ¤ì˜ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤
- **Observer Pattern**: ExperimentRecorderì˜ ìë™ ìˆ˜ì§‘

## ê³ ê¸‰ ê¸°ëŠ¥

### Model Registry

```python
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
research.list_models()

# ëª¨ë¸ ìƒì„±
from research.models.pretrained import ModelRegistry
model = ModelRegistry.create('resnet50', num_classes=100)
```

### Comparison System

```python
from research.comparison import ComparisonManager
from research.comparison.comparators import (
    PerformanceComparator,
    EfficiencyComparator,
    SpeedComparator
)

manager = ComparisonManager()
manager.add_comparator(PerformanceComparator('accuracy'))
manager.add_comparator(EfficiencyComparator('accuracy'))
manager.add_comparator(SpeedComparator())

results = manager.compare(experiment_results)
manager.print_summary(results)
```

### 1-Channel ë°ì´í„° ì§€ì›

Mel-spectrogramì´ë‚˜ grayscale ì´ë¯¸ì§€ ê°™ì€ 1ì±„ë„ ë°ì´í„°:

```python
config = {
    'num_classes': 10,
    'in_channels': 1,  # 1ì±„ë„ ì…ë ¥
    'learning_rate': 1e-4
}

exp = research.Experiment(config)
exp.setup(model_name='resnet18', data_module=mel_datamodule)
```

## í…ŒìŠ¤íŠ¸

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/

# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/unit/

# íŠ¹ì • ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
pytest tests/unit/test_metrics.py

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
pytest tests/ --cov=research --cov-report=term-missing
```

## ì˜ˆì œ

`examples/` ë””ë ‰í† ë¦¬ì˜ ì˜ˆì œë“¤:

- `quickstart.py` - ì „ì²´ ì›Œí¬í”Œë¡œìš° ë°ëª¨
- `test_metric_system.py` - ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ ì‚¬ìš©ë²•
- `test_visualization.py` - ì‹œê°í™” ê¸°ëŠ¥
- `test_task_strategies.py` - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì „ëµ
- `test_comparison_system.py` - ëª¨ë¸ ë¹„êµ

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

CIFAR-10 ë°ì´í„°ì…‹ ê¸°ì¤€:

| ëª¨ë¸ | Feature Extraction | Fine-tuning | íŒŒë¼ë¯¸í„° ìˆ˜ | ì¶”ë¡  ì‹œê°„ |
|------|-------------------|-------------|------------|----------|
| ResNet18 | 85.2% | 92.1% | 11.7M | 8ms |
| ResNet50 | 87.3% | 93.5% | 25.6M | 15ms |
| VGG16 | 84.1% | 91.8% | 138M | 12ms |

## ë¼ì´ì„ ìŠ¤

MIT License

## ê¸°ì—¬í•˜ê¸°

Issuesì™€ Pull RequestsëŠ” ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤!

## ì €ì

KTB AI Research Team

---

**Note**: ì´ í”„ë ˆì„ì›Œí¬ëŠ” ì´ì „ `ktb_dl_research`ì™€ `ml_framework`ë¥¼ í†µí•©í•˜ì—¬ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤.